{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<p align=center>ðŸ¤–Zoidberg 2.0ðŸ“ˆ</p>\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "RBf2xaTaFVbu"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mounting the dataset"
   ],
   "metadata": {
    "id": "Gak9x-cpgjPC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The google collab environment isn't persistent, we therefore need to mount our drive (personal one, a shortcut to the shared-with-me folder containing the dataset needs to be created) to access the dataset."
   ],
   "metadata": {
    "id": "lgWVhpsojz30"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "base_dir = \"/content/drive/MyDrive/ZoidBerg2.0 - T-Dev-810/chest_Xray\""
   ],
   "metadata": {
    "id": "tnjTuE5tHESm",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1715590366446,
     "user_tz": -120,
     "elapsed": 39464,
     "user": {
      "displayName": "Pierre Mauger",
      "userId": "11898204358999209389"
     }
    },
    "outputId": "3257d21b-2916-435f-84f8-63c24fc41f2f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading the dataset"
   ],
   "metadata": {
    "id": "OpQLHnu7lrAx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that the dataset is accessible, we need to load its images that will be used to [train, validate and test](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets) our algorithm."
   ],
   "metadata": {
    "id": "nVswhVReXVEI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## When to load"
   ],
   "metadata": {
    "id": "e9t0wegMUF2g"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This could either be done\n",
    "- at the start (pre-loading), allowing\n",
    "  - to load everything at once faster through multiprocessing\n",
    "  - to access images faster since we just need to read from the disk everytime it's needed\n",
    "  - consistency on epochs as they'll use exactly the same data every time\n",
    "  - a clear separated code flow\n",
    "- everytime an image is needed (on-the-fly loading), permitting\n",
    "  - real-time data augmentation, if the dataset is updated while the model is trained\n",
    "  - memory efficiency, large datasets may not fit into the RAM if loaded all at once\n",
    "\n",
    "Given the benefits differencial, one would choose the former. That's however if it wasn't for the fact that our dataset is 1.16GB of **compressed** images that will, once decompressed, weight in total more than what an usual 12-15GB CPU/GPU RAM can handle.\n",
    "\n",
    "We therefore chose to opt for the later through a technique known as \"batch processing\", where the large initial data is split evenly into smaller \"batches\" of images that are decompressed/compressed in the RAM when needed."
   ],
   "metadata": {
    "id": "o8Dc6TtErs3k"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset analysis"
   ],
   "metadata": {
    "id": "coC3qpzfUJ1y"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next step is to analyse the provided dataset's folder architecture:\n",
    "\n",
    "- train\n",
    "  - PNEUMONIA\n",
    "    - 3875 elements\n",
    "  - NORMAL\n",
    "    - 1341 elements\n",
    "- val\n",
    "  - PNEUMONIA\n",
    "    - 8 elements\n",
    "  - NORMAL\n",
    "    - 8 elements\n",
    "- test\n",
    "  - PNEUMONIA\n",
    "    - 390 elements\n",
    "  - NORMAL\n",
    "    - 234 elements\n",
    "\n",
    "for a total of 5856 elements where elements are .jpeg images of various sizes ranging from 384 to 2916 in width and 127 to 2713 in height.\n",
    "\n",
    "Additionally, the PNEUMONIA folders' images' names contain either \"virus\" or \"bacteria\", allowing us to load/use those separately for a more precise pneumonia diagnostic:\n",
    "- train\n",
    "  - bacteria 2530\n",
    "  - virus 1345\n",
    "- val\n",
    "  - bacteria 8\n",
    "  - virus 0\n",
    "- test\n",
    "  - bacteria 242\n",
    "  - virus 148\n",
    "\n",
    "We can note that there is significantly more images pneumonia originating from bacterias. This data imbalance could lead into the model being biased into leaning results more toward bacteria pneumonia by default."
   ],
   "metadata": {
    "id": "Ga32ix4wrFWD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data augmentation"
   ],
   "metadata": {
    "id": "dgMLdaFebqMp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To fight this potential bias, a technique know as [data augmentation](https://en.wikipedia.org/wiki/Data_augmentation) can help with generating new data from existing data.\n",
    "\n",
    "The benefits, among others, are:\n",
    "- Enhanced model performance\n",
    "\n",
    "  Data augmentation techniques help enrich datasets by creating many variations of existing data. This provides a larger dataset for training and enables a model to encounter more diverse features. The augmented data helps the model better generalize to unseen data and improve its overall performance in real-world environments.\n",
    "\n",
    "- Reduced data dependency\n",
    "\n",
    "  Data augmentation is a useful technology in medical imaging because it helps improve diagnostic models that detect, recognize, and diagnose diseases based on images. The creation of an augmented image provides more training data for models, especially for rare diseases that lack source data variations. The production and use of synthetic patient data advances medical research while respecting all data privacy considerations.\n",
    "\n",
    "- Mitigate overfitting in training data\n",
    "\n",
    "  Overfitting is the undesirable ML behavior where a model can accurately provide predictions for training data but it struggles with new data. In contrast, data augmentation provides a much larger and more comprehensive dataset for model training. It makes training sets appear unique to deep neural networks, preventing them from learning to work with only specific characteristics.\n",
    "\n",
    "Data augmentation is a central technique in computer vision tasks. It helps create diverse data representations and tackle class imbalances in a training dataset.\n",
    "\n",
    "The first usage of augmentation in computer vision is through position augmentation. This strategy crops, flips, or rotates an input image to create augmented images. Cropping either resizes the image or crops a small part of the original image to create a new one. Rotation, flip, and resizing transformation all alter the original randomly with a given probability of providing new images.\n",
    "\n",
    "Another usage of augmentation in computer vision is in color augmentation. This strategy adjusts the elementary factors of a training image, such as its brightness, contrast degree, or saturation. These common image transformations change the hue, dark and light balance, and separation between an image's darkest and lightest areas to create augmented images.\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:850/1*ae1tW5ngf1zhPRyh7aaM1Q.png'>\n"
   ],
   "metadata": {
    "id": "qb5HEWKobwSV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Image processing libraries"
   ],
   "metadata": {
    "id": "DA6wOuTgURIH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following task, since there is so many images to load, is deciding which python image processing library to pick for the fastest loading.\n",
    "\n",
    "[This article](https://learnopencv.com/efficient-image-loading/) details benchmarking code to compare the 2 most popular image processing libraries in python (Pillow and OpenCV), one of their optimization fork (Pillow-SIMD) and a fourth optimized jpeg-specialized one (TurboJPEG).\n",
    "\n",
    "As we can see from its results, TurboJPEG is the most efficient library and is specialized in the .jpeg format which we use, making it the perfect choice.\n",
    "\n",
    "<img src='https://learnopencv.com/wp-content/uploads/2020/06/mean-median-rgb.png'>"
   ],
   "metadata": {
    "id": "j45_Iq2y_u5I"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Code"
   ],
   "metadata": {
    "id": "fqinB4ZAUpS9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "sets = [\"train\", \"val\", \"test\"]\n",
    "types = [\"PNEUMONIA\", \"NORMAL\"]\n",
    "images = {}\n",
    "for set_name in sets:\n",
    "    images[f\"{set_name}_normal\"] = []\n",
    "    images[f\"{set_name}_bacteria\"] = []\n",
    "    images[f\"{set_name}_virus\"] = []\n",
    "lowest_width, lowest_height, highest_width, highest_height = float('inf'), float('inf'), 0, 0\n",
    "\n",
    "print(f\"loading images from {base_dir}\")\n",
    "size = 0\n",
    "for set_name in sets:\n",
    "    for type_name in types:\n",
    "        path = os.path.join(base_dir, set_name, type_name)\n",
    "        for filename in os.listdir(path):\n",
    "            if filename.endswith(\".jpeg\"):\n",
    "                with Image.open(os.path.join(path, filename)) as image:\n",
    "                    width, height = image.size\n",
    "                    size += width * height * (1 if image.mode == \"L\" else 3)\n",
    "                    lowest_width = min(lowest_width, width)\n",
    "                    lowest_height = min(lowest_height, height)\n",
    "                    highest_width = max(highest_width, width)\n",
    "                    highest_height = max(highest_height, height)\n",
    "                    image.load()\n",
    "                    if \"bacteria\" in filename:\n",
    "                        images[f\"{set_name}_bacteria\"].append(image)\n",
    "                    elif \"virus\" in filename:\n",
    "                        images[f\"{set_name}_virus\"].append(image)\n",
    "                    else:\n",
    "                        images[f\"{set_name}_normal\"].append(image)\n",
    "print(size)\n",
    "total = 0\n",
    "for set_type in images:\n",
    "    print(f\"{set_type}: {len(images[set_type])} images loaded\")\n",
    "    total += len(images[set_type])\n",
    "print(f\"total: {total} images loaded\")\n",
    "print(f\"lowest width: {lowest_width}\")\n",
    "print(f\"highest width: {highest_width}\")\n",
    "print(f\"lowest height: {lowest_height}\")\n",
    "print(f\"highest height: {highest_height}\")"
   ],
   "metadata": {
    "id": "0XNbM8HomB_v",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1715588621358,
     "user_tz": -120,
     "elapsed": 366833,
     "user": {
      "displayName": "Baptiste Lemonnier",
      "userId": "06095563635647499760"
     }
    },
    "outputId": "081fa70f-8f07-46a5-fa91-4b0f31e5c537"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convolutional Neural Networks"
   ],
   "metadata": {
    "id": "1gJCVv-c86hZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Convolutional Neural Networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNNs) are a class of [deep learning](https://en.wikipedia.org/wiki/Deep_learning) algorithms that are particularly effective for image analysis tasks, making them ideal for pneumonia recognition in radiographic images:\n",
    "\n",
    "- Feature Learning: Traditional image processing techniques require manual extraction of features. CNNs can automatically learn and extract features from images, which is beneficial in medical imaging where defining what constitutes a feature can be complex.\n",
    "\n",
    "- Hierarchical Pattern Recognition: CNNs work by recognizing patterns in a hierarchical manner. Lower layers may recognize simple features like edges and lines, while deeper layers combine these simple features to recognize more complex structures. This is particularly useful in pneumonia detection where the disease can manifest as a variety of patterns in the lungs.\n",
    "\n",
    "- Translation Invariance: Once a CNN learns a feature, it can recognize that feature anywhere in the image. This is crucial in pneumonia detection as the infection can occur in different parts of the lungs.\n",
    "\n",
    "- Robustness to Noise and Variations: Medical images can often contain noise and can vary due to different imaging conditions. CNNs are robust to such variations and can still perform well.\n",
    "\n",
    "- End-to-End Training: With CNNs, the entire model is trained in an end-to-end fashion. This means that the raw pixel values of the image are input to the model, and the model learns to extract features and make predictions all by itself. This eliminates the need for manual feature extraction or selection.\n",
    "\n",
    "- Performance: CNNs have been proven to perform exceptionally well on image classification tasks. They have achieved state-of-the-art results in many medical imaging tasks, including pneumonia detection.\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:1400/1*Fw-ehcNBR9byHtho-Rxbtw.gif'>\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "8ZXhVhZI8-6N"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Handling images of different sizes"
   ],
   "metadata": {
    "id": "hJflaaz9l0j_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As mentionned before, the dataset is made of images with greatly various sizes. As we can see from [this article](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8700246/), it will have an impact on the CNN performance. Which is why we need to correctly [handle this problem](https://wandb.ai/ayush-thakur/dl-question-bank/reports/How-to-Handle-Images-of-Different-Sizes-in-a-Convolutional-Neural-Network--VmlldzoyMDk3NzQ) (note that the following methods aren't considered for anything else than having a CNN work in the first place, data augmentation could be employed only after that):"
   ],
   "metadata": {
    "id": "VNkBGR6_2-mp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Resizing"
   ],
   "metadata": {
    "id": "kYCcR3qYR0Xc"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Resizing images to a common single size is a popular approach in processing data for medical imagery CNNs due to several key reasons.\n",
    "\n",
    "Firstly, standardizing image dimensions ensures uniformity in input data, facilitating the training process and improving model performance by reducing variations in feature representation.\n",
    "\n",
    "Secondly, it streamlines computational complexity, as fixed-size inputs simplify the architecture design and optimize memory usage during training and inference.\n",
    "\n",
    "Multiple papers have extensively researched on [The effect of image resizing on CNN performance](https://isprs-archives.copernicus.org/articles/XLVI-4-W5-2021/501/2021/isprs-archives-XLVI-4-W5-2021-501-2021.pdf/), clearly depicted here![picture](https://drive.google.com/uc?id=1Ln8OffmMfiqgbP5NUmSXOxJGGL9DHDYf)\n",
    "\n",
    "which makes [Learning to Resize Images for Computer Vision Tasks](https://arxiv.org/pdf/2103.09950.pdf) important.\n",
    "\n",
    "In the case of our dataset, the pixels corresponding to a potential small pneumonia could disappear during a downsize or pixels corresponding to a bone/the lung/noise could be distorded during an upsize resulting in the CNN giving out false negatives or positives.\n",
    "\n",
    "The amount of noise present on some images (\"R\" at the top left and little white lines on the sides for all images as well as some having the hour or some other medical informations text at the corners) could also be amplified by an upsize making it more difficult for the CNN to identify key features.\n",
    "\n",
    "In conclusion, while resizing remains a popular and valuable preprocessing step for handling datasets of images of various sizes/ratios, its implementation must be approached with care and consideration for its potential impact on diagnostic accuracy in the case of medical images."
   ],
   "metadata": {
    "id": "TU5DS2WZR9D_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cropping"
   ],
   "metadata": {
    "id": "RD8S_OgHSBmU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similar to resizing, cropping facilitates the standardization of input size for CNNs. By cropping images to a consistent size, variations in input dimensions are minimized, which simplifies the training process and improves model performance.\n",
    "\n",
    "One of the primary advantages of cropping is the ability to extract specific regions of interest within an image. By eliminating extraneous information (such as the previous ones I've mentionned), cropping enables a more focused and efficient analysis process.\n",
    "\n",
    "However, as the images are of different sizes, ratios and focus (not all of them are centered on the backbone and some patients were a bit tilted), efficient constant cropping might prove difficult to the point where a dynamic CNN approach could be considered but would add an unnecesarry layer of difficulty.\n",
    "\n",
    "<img src='https://aitorshuffle.github.io/images/picon_crop_2019_fig4_abstract.png'>\n"
   ],
   "metadata": {
    "id": "k1GPyMPMSFGO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inherent Network Property"
   ],
   "metadata": {
    "id": "I9jKdRQeSGEI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Fully Convolutional Networks](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf) (FCNs) are a type of neural network architecture specifically designed for semantic segmentation tasks, where the goal is to label each pixel in an image with a corresponding class label (for example: pneumonia pixels).\n",
    "\n",
    "<img src='https://production-media.paperswithcode.com/methods/new_alex-model.jpg'>\n",
    "\n",
    "Let's break down the key components and concepts associated with FCNs:\n",
    "\n",
    "- Convolutional Layers:\n",
    "\n",
    "  FCNs primarily consist of convolutional layers, which are responsible for learning hierarchical features from the input image. These layers apply convolution operations to the input image, extracting features at different spatial resolutions.\n",
    "- Pooling Layers:\n",
    "\n",
    "  Pooling layers are used in FCNs for downsampling the feature maps, reducing their spatial dimensions while retaining important features. Common pooling operations include **max pooling and average pooling**.\n",
    "- Upsampling Layers:\n",
    "\n",
    "  Upsampling layers are used to increase the spatial dimensions of the feature maps. This helps in recovering the spatial information lost during downsampling and enables the network to produce segmentation maps with the same resolution as the input image.\n",
    "- Avoidance of Dense Layers:\n",
    "\n",
    "  Unlike traditional neural network architectures, FCNs avoid using dense (fully connected) layers. Dense layers would require a fixed-size input, which is not suitable for segmentation tasks where the input image can have varying dimensions. By using only convolutional and upsampling layers, FCNs can handle images of different sizes efficiently.\n",
    "- Locally Connected Layers:\n",
    "\n",
    "  FCNs only use locally connected layers, meaning each output feature depends only on a small region of the input image. This property allows FCNs to **handle images of different sizes without the need for resizing or cropping**.\n",
    "- Downsampling Path and Upsampling Path:\n",
    "\n",
    "  FCNs are typically structured with a downsampling path and an upsampling path. The downsampling path, often composed of convolutional and pooling layers, captures the context and global information of the input image. The upsampling path, composed of upsampling and convolutional layers, helps in localizing objects by gradually increasing the spatial resolution of the feature maps.\n",
    "- Skip Connections:\n",
    "\n",
    "  Skip connections are connections between layers at the same spatial resolution in the downsampling and upsampling paths. These connections help preserve fine details and spatial information that may be lost during downsampling. By combining features from different resolutions, skip connections improve the segmentation accuracy of FCNs.\n",
    "\n",
    "In summary, FCNs are tailored for semantic segmentation tasks, leveraging convolutional layers, pooling, and upsampling while avoiding dense layers. Their locally connected design and use of skip connections enable them to handle images of varying sizes and capture fine details necessary for accurate pixel-wise labeling."
   ],
   "metadata": {
    "id": "Q4-2_gsmSL66"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Global Average Pooling (GAP) and Global Max Pooling (GMP)](https://blog.paperspace.com/global-pooling-in-convolutional-neural-networks/) are fundamental techniques within convolutional neural networks (CNNs), frequently employed for feature extraction and dimensionality reduction, notably in image classification tasks.\n",
    "\n",
    "<img src='https://you359.github.io/images/contents/cam_gap.png'>\n",
    "\n",
    "GAP calculates the average activation across each feature map, effectively condensing spatial information into a single value per map. Conversely, GMP selects the maximum activation within each feature map, discarding the rest. These operations serve to reduce the spatial dimensions of the feature maps, facilitating subsequent processing.\n",
    "\n",
    "What makes GAP and GMP particularly advantageous is their flexibility in handling datasets comprising images of varying sizes. Unlike fully connected layers that require fixed input dimensions, GAP and GMP operate independently of image size. This means CNN architectures employing these pooling techniques can seamlessly process images of different resolutions without the need for resizing or cropping, enhancing their adaptability to diverse datasets.\n",
    "\n",
    "Moreover, both GAP and GMP play a pivotal role in enhancing computational efficiency. By condensing feature maps into single values, they significantly reduce the number of parameters in the network, leading to faster training and inference times. Additionally, this reduction in dimensionality helps mitigate overfitting, promoting generalization performance.\n",
    "\n",
    "In summary, GAP and GMP offer a robust solution for feature extraction and dimensionality reduction in CNNs, especially in the context of image classification tasks. Their ability to handle images of varying sizes and enhance computational efficiency makes them indispensable components of modern CNN architectures."
   ],
   "metadata": {
    "id": "1fseWjQhO4Ig"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data formats"
   ],
   "metadata": {
    "id": "wAuLJyQw_rWu"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So far we've only analysed the data itself and the provided folder architecture, but what about the nature itself of the dataset being reading directly from files ? Isn't there is machine learning dataset format specialized in reading more data more efficiently ?\n",
    "\n",
    "There is, and according to this elaborate [article](https://servicedesk.surf.nl/wiki/display/WIKI/Best+Practice+for+Data+Formats+in+Deep+Learning),\n",
    "<img src='https://servicedesk.surf.nl/wiki/download/attachments/56295498/benchmark_ImageNet10k_results.png?version=2&modificationDate=1677235283181&api=v2'>\n",
    "HDF5 format would be the best choice for us as it's made of \"intuitive array-based data like images\" that isn't large enough (< 100GB) to justify LMDB.\n",
    "\n",
    "We'll be ignoring Petastorm because of its complexity, TFRecords because it's not generalized for native use with Pytorch, ZIP/TAR because it would add an extra layer of uncompressing and we aren't necessarily concerned by disk space with google drive.\n",
    "\n",
    "The final options are leaving the jpeg dataset as-is since, given the size of the initial dataset, the loading time is negligible or use HDF5 once the rest of the code is done to see if it makes a difference."
   ],
   "metadata": {
    "id": "lhvZhCr3AE4l"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# First Iteration"
   ],
   "metadata": {
    "id": "xcW_nkoiVpdk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "network.py"
   ],
   "metadata": {
    "id": "Geph3CR5W_L8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 64 * 28 * 28)  # Flatten the output of conv3 layer\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "id": "cpMeGoS0WzMb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "train.py"
   ],
   "metadata": {
    "id": "FQxBszY-Wv5v"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class FilteredImageFolder(ImageFolder):\n",
    "    def __init__(self, root, transform=None, target_transform=None, filter_keywords=None):\n",
    "        super(FilteredImageFolder, self).__init__(root, transform, target_transform)\n",
    "        self.filter_keywords = filter_keywords\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        if self.filter_keywords is not None:\n",
    "            filename = os.path.basename(path)\n",
    "            for keyword in self.filter_keywords:\n",
    "                if keyword in filename:\n",
    "                    return sample, self.filter_keywords[keyword]\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return sample, target\n",
    "\n",
    "filter_keywords = {'virus': 1, 'bacteria': 2}\n",
    "\n",
    "# Load datasets using ImageFolder\n",
    "train_dataset = FilteredImageFolder(root='datasets/train', transform=transform, filter_keywords=filter_keywords)\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "#print classes\n",
    "for i in range(len(train_dataset.classes)):\n",
    "    print(i, train_dataset.classes[i])\n",
    "\n",
    "# Print the number of samples in each dataset and each class\n",
    "print('Dataset\\t', 'Total\\t', train_dataset.classes[0], '', train_dataset.classes[1])\n",
    "print('Train:\\t', len(train_dataset), '\\t',  train_dataset.targets.count(0), '\\t',  train_dataset.targets.count(1), '\\n')\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device, '\\n')\n",
    "\n",
    "# Create an instance of the model\n",
    "num_classes = len(train_dataset.classes)\n",
    "model = CNN(num_classes=num_classes).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "train_loss = []\n",
    "accuracy_total_train = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(total=len(train_loader), desc='Epoch {}/{}'.format(epoch+1, num_epochs), position=0, leave=True)\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update the weights\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Calculate the accuracy\n",
    "        outputs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        accuracy_total_train.append(torch.sum(preds == labels.data).item() / float(inputs.size(0)))\n",
    "\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    progress_bar.close()\n",
    "    print('Loss: {:.4f}'.format(epoch_loss),\n",
    "          'Accuracy: {:.4f}'.format(sum(accuracy_total_train) / len(accuracy_total_train)))\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "print('Saving the model...')\n",
    "torch.save(model.state_dict(), 'modelv0.pth')\n",
    "print('Model saved as model.pth')\n"
   ],
   "metadata": {
    "id": "L9XP1wENVqMM",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "status": "error",
     "timestamp": 1715594474310,
     "user_tz": -120,
     "elapsed": 460,
     "user": {
      "displayName": "FranÃ§ois PARMENTIER",
      "userId": "18444316562309287408"
     }
    },
    "outputId": "f8f7101a-466e-4204-b5ab-cefef0642907"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "test.py"
   ],
   "metadata": {
    "id": "oaD1rN6pXCmQ"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Rf2Tw3SeXFHF"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "colab": {
   "provenance": [],
   "toc_visible": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
